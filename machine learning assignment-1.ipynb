{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Code: DA-AG-012\n",
        "##Assignment\n",
        "#Decision Tree | Assignment\n",
        "\n",
        "###Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "- A Decision Tree is a method used by computers to make decisions in a simple and step-by-step way. It is called a “tree” because it starts from one main point (called the root) and then branches out just like a real tree.\n",
        "In real life, we make decisions by asking ourselves questions. A decision tree does the same. It asks a question at each step and follows the answer until it reaches a final decision. It is mostly used for classification.\n",
        "\n",
        "- how does it work?\n",
        " - Is it raining?\n",
        "   - Yes → Wear a raincoat\n",
        "   - No → Is it cold?\n",
        " - Yes → Wear a jacket\n",
        " - No → Wear a T-shirt\n",
        "\n",
        "- steps:-\n",
        "  - start with the dataset\n",
        "  - find the best question to ask first\n",
        "  - split the data based on the answer\n",
        "  - keep repeating\n",
        "  - make predictions\n",
        "\n",
        "- Advantage:-\n",
        " - Very easy to understand and explain.\n",
        " - Looks like a flowchart — no complicated math.\n",
        " - Works with all types of data (numbers, text, yes/no, etc.).\n",
        " - Can handle missing data sometimes.\n",
        "\n",
        "- Disadvantage:-\n",
        " - Trees can become too long or complex if not controlled.\n",
        " - Small changes in data can change the tree completely.\n",
        "\n",
        "\n",
        "\n",
        "### Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- ###[gini impurity ]\n",
        "- Gini impurity is the concept which explains how often a randomely choosen item would be incorrectly classified if it was labeled based oon class distribution data.\n",
        "- formula:- gini=1- square of probability of each class\n",
        "  - example:- say we have 10 student in class\n",
        "     - 6 of class A\n",
        "     - 4 of class B\n",
        "     - ginni=1-0.6square+0.4square\n",
        "     - 1-0.36+0.16=0.48\n",
        "\n",
        "###[entropy]\n",
        "- entropy tell us how much uncertainity there is in data.\n",
        "- formula:- entropy=-(p1log2(p1))\n",
        " - example:- say we have 10 studets in class\n",
        "    - 6 of class A\n",
        "    - 4 lf class B\n",
        "    - entropy = -(0.6log2(0.6)+0.4log2(0.4))\n",
        "\n",
        "###[how do they affect the data]\n",
        "- Both are just tools to help the tree figure out how to split the data.\n",
        "- The better the split (more pure), the better the decision tree can make predictions.\n",
        "--------------------------------------------------------------------------------\n",
        "  \n",
        "### Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "###[pre-pruning]\n",
        "- pruning means cutting  like branches of trees so that it doesnt become dense so in machine learning we cut down old unnecessary data which is stopping it from performing well on new data.\n",
        "\n",
        "###[post-pruning]\n",
        "- post-pruning is when you let the tree grow by cutting old unnecessary data.\n",
        "\n",
        "###Differenec between pre-pruning and post-pruning\n",
        "- main difference between pre-pruning and post-pruning is Pre-Pruning stops the tree from growing too much during training, while Post-Pruning cuts back the tree after it's fully grown to reduce overfitting.\n",
        "\n",
        "###[practicle advantage of pre-prunning]\n",
        "- Pre-Pruning: Speeds up training and saves memory by limiting tree growth early.\n",
        "\n",
        "###[practicle advantage of post-pruning]\n",
        "- Post-Pruning: Improves model accuracy by removing overfitting branches after training.\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "###Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- information gain is a feature that tells us how much each feature helps us reducing or removig confusion while spliting the data in decision tree.\n",
        "\n",
        "- importance:-\n",
        "\n",
        "  - It helps the tree figure out which question actually makes the data clearer.\n",
        "  - The higher the info gain, the better the split — less mess, more sense.\n",
        "  - It basically tells the tree,  this feature helps best- use it\n",
        "  - It stops the tree from making dumb or random splits.\n",
        "  - Makes the decision tree smarter by picking the most useful info first.\n",
        "\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "###Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "- some of common real-world applications of decision trees are\n",
        " - education- predicitng weither a student will pass/fail based on its study timming,previous marks etc.\n",
        " - finance-deciding if one should get loan or credit card.\n",
        " - E-commerce - suggesting products based on customer history and search habbits.\n",
        " - marketing- giving ads based on their history.\n",
        "\n",
        "###[Advantage]\n",
        "- Easy to understand\n",
        "- Works with both numbers and categories\n",
        "-  Needs little data prep\n",
        "- Can handle missing values\n",
        "- Fast to train\n",
        "\n",
        "###[Disadvantage]\n",
        "- can overfit\n",
        "- unstable\n",
        "- not great with too many features\n",
        "- can be biased\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "```Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV)\n",
        "```\n",
        "\n",
        "### Write a Python program to:\n",
        "###● Load the Iris Dataset\n",
        "###● Train a Decision Tree Classifier using the Gini criterion\n",
        "###● Print the model’s accuracy and feature importances\n",
        "###(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4OL5QhbRk_UX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aj7UbWHSkCtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d54bf42-793f-47cd-d157-7e4f317a5d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm) : 0.0075\n",
            "sepal width (cm) : 0.0188\n",
            "petal length (cm) : 0.0758\n",
            "petal width (cm) : 0.8978\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "#loading data set\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Creating  the Decision Tree model using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# making accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print which features were most important for the decision\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, score in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(name, \":\", round(score, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "#####● Load the Iris Dataset\n",
        "#####● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "Inf3OKRcOopN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "model_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_limited.fit(X_train, y_train)\n",
        "y_pred_limited = model_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "#train full tree\n",
        "model_full = DecisionTreeClassifier(random_state=42)\n",
        "model_full.fit(X_train, y_train)\n",
        "y_pred_full = model_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "#Print both accuracies\n",
        "print(\"Accuracy with max_depth=3:\", round(accuracy_limited, 2))\n",
        "print(\"Accuracy with full tree:\", round(accuracy_full, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47sRbS9gOHvo",
        "outputId": "bb5b6404-2110-4c7e-d5cd-6d8bd2bc918e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with full tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#####● Load the Boston Housing Dataset\n",
        "#####● Train a Decision Tree Regressor\n",
        "#####● Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "MyiJWGT8PRCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#question-9 Write a Python program to:\n",
        "#####● Load the Iris Dataset\n",
        "#####● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#####● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "rxISbdKSPw1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "#loading data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Create model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Setting parameters\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV\n",
        "grid = GridSearchCV(model, params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# print best model\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "y_pred = grid.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhrBLNhsQC-u",
        "outputId": "9592e890-d9a7-4b97-ca75-31fea8eb7df1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': 4, 'min_samples_split': 4}\n",
            "Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "#####● Handle the missing values\n",
        "#####● Encode the categorical features\n",
        "#####● Train a Decision Tree model\n",
        "#####● Tune its hyperparameters\n",
        "#####● Evaluate its performance\n",
        "###And describe what business value this model could provide in the real-world setting.\n"
      ],
      "metadata": {
        "id": "HpUmChzFQvPO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTHr9xwVQgy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}