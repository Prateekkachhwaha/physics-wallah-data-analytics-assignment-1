{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Assignment Code: DA-AG-014**\n",
        "#**Ensemble Learning | Assignment**\n",
        "\n",
        "##Question-1 What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- esemble learning in mahine learning is an benifitial and usefull technique whihc takes outputs or predictions from multiple models to give us more trained and better outputs\n",
        "- its usage and purpose:-\n",
        "  - Improving Accuracy of Predictions\n",
        "  - Reducing Overfitting and Underfitting\n",
        "  - Handling Complex Datasets\n",
        "\n",
        "- the core concept is that a group of models (learners) can collaborate to solve a problem more accurately and robustly than one model alone.\n",
        "\n",
        "---\n",
        "\n",
        "##Question-2 What is the difference between Bagging and Boosting?\n",
        "- 1) bagging trains multiple models in one while boosting trains them seperatly in sequence.\n",
        "- 2) bagging reduces variance while boosting reduces biasness.\n",
        "- 3) bagging reduces time taking while boosting takes time.\n",
        "- 4) Bagging treats all models equally, Boosting gives more weight to better models.\n",
        "- 5) Bagging builds models independently, Boosting builds models that learn from each other.\n",
        "- 6) Bagging reduces model instability, Boosting increases model accuracy.\n",
        "- 7) Bagging is easier to parallelize, Boosting is inherently sequential.\n",
        "- 8) Bagging typically uses homogeneous models, Boosting can use adaptive model weights.\n",
        "- 9) Bagging improves stability, Boosting improves learning depth.\n",
        "- 10) Bagging starts with equal model importance, Boosting gradually shifts focus to stronger models.\n",
        "\n",
        "---\n",
        "\n",
        "##Question-3 What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Bootstrap sampling is a statistical technique where we create multiple random samples from a dataset with replacement.\n",
        "This means the same data point can appear multiple times in one sample, or not at all.\n",
        "\n",
        "- In Bagging (Random Forest):\n",
        " - Each base model (e.g., decision tree) is trained on a different bootstrap sample.\n",
        " - This introduces diversity among models by giving them different training data.\n",
        " - The final prediction is made by combining (e.g., voting or averaging) all the models' outputs.\n",
        " - This helps to reduce variance and makes the ensemble model more stable and accurate.\n",
        "\n",
        "---\n",
        "\n",
        "##Question-4 What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "- In Bagging methods like Random Forest, each base learner (e.g., decision tree) is trained on a bootstrap sample — a random sample with replacement from the original dataset.Because of sampling with replacement, about 63% of the original data points typically end up in each bootstrap sample, and the remaining ~37% are not included in that sample.\n",
        "\n",
        "- how oob score used is written below:-\n",
        " - Each tree in the forest is trained on a random sample of the data (with replacement).\n",
        " - Some data points are left out of that sample — these are called OOB samples for that tree.\n",
        " - After training, we use the tree to predict the OOB samples it didn't see.\n",
        " - We collect predictions from all trees that didn’t train on each data point.\n",
        " - We compare these OOB predictions to the real answers.\n",
        " - The result is the OOB score, which tells us how accurate the model is.\n",
        "\n",
        "---\n",
        "\n",
        "##Question-5 Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- the difference of single decision tree and random forest is written below:-\n",
        "\n",
        "1. **Decision Tree** checks which features are most useful in **one tree**, while **Random Forest** looks at **many trees** to decide.\n",
        "2. **Decision Tree** uses **all the data**, while **Random Forest** uses **random parts** of the data and features.\n",
        "3. **Decision Tree** can be **easily confused by noise**, while **Random Forest** is **more stable** and accurate.\n",
        "4. **Decision Tree** might think some features are important **just by chance**, while **Random Forest** helps fix that by averaging.\n",
        "5. **Decision Tree** gives you results **quickly**, but they might not be very **trustworthy**, while **Random Forest** takes longer but is **more reliable**.\n",
        "6. **Decision Tree** shows what’s important for **one path**, while **Random Forest** shows what’s important **overall**.\n",
        "7. **Decision Tree** can change a lot if the data changes a little, but **Random Forest** stays **more consistent**.\n",
        "8. **Decision Tree** is easier to understand, but **Random Forest** is **better for real-world problems**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zkJb3etRa6Qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-6 Write a Python program to:\n",
        "- Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "- Train a Random Forest Classifier\n",
        "-  Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "bNu1k6PypwgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLyfBxPiazBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf53b0c-4fa9-46ee-8ac0-a148e4b96b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer.target)\n",
        "\n",
        "# Train data\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "feature_importances = pd.Series(model.feature_importances_, index=cancer.feature_names)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 most important features:\")\n",
        "print(feature_importances.nlargest(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## question-7 Write a Python program to:\n",
        " - Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        " - Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "wYvwP77RtTxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Train a single Decision Tree Classifier\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "single_tree_pred = single_tree.predict(X_test)\n",
        "single_tree_accuracy = accuracy_score(y_test, single_tree_pred)\n",
        "\n",
        "#Train the data\n",
        "bagging_tree = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                n_estimators=10, random_state=42)\n",
        "bagging_tree.fit(X_train, y_train)\n",
        "bagging_tree_pred = bagging_tree.predict(X_test)\n",
        "bagging_tree_accuracy = accuracy_score(y_test, bagging_tree_pred)\n",
        "\n",
        "#Printing the data\n",
        "print(f\"Accuracy of single Decision Tree: {single_tree_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {bagging_tree_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "6rafUNWev6UQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f83e1a1-3b7e-4b72-c9c4-9124a011af18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#quetion-8 Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "Y1V-nbocSfoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "#Load the dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = pd.Series(cancer.target)\n",
        "\n",
        "#Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "#Creating a Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "#Performing GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#Print the best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best accuracy score on training data: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "#Evaluate the model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#Printing the data\n",
        "print(f\"Accuracy of the best model on the test set: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HMVR-KFSJVx",
        "outputId": "afe5637a-34e3-464a-9232-747d6127c2b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'n_estimators': 200}\n",
            "Best accuracy score on training data: 0.9522\n",
            "Accuracy of the best model on the test set: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#question-9 Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "rMrCn_tXTrmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#loading the data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "#Spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Train a Bagging data\n",
        "bagging_reg = BaggingRegressor(random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_pred = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "\n",
        "#Training data\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "#Printing Mean Squared Errors\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {bagging_mse:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {rf_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJQ0arZSTjbr",
        "outputId": "ae891357-3277-4d3a-ba92-cdc3869bebdb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2862\n",
            "Mean Squared Error of Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###question-10 You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.\n",
        "- Explain your step-by-step approach to:\n",
        " - Choose between Bagging or Boosting\n",
        " - Handle overfitting\n",
        " - Select base models\n",
        " - Evaluate performance using cross-validation\n",
        " - Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "TXy4dlnxVYrV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYymBwINUQqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}