{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Code: DA-AG-016**\n",
        "#KNN & PCA | Assignment\n",
        "\n",
        "###Question-1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "- K-Nearest Neighbors is an  machine learning algorithm that can be used for both classification and regression tasks. It is not pre defined it learns on the place instantly  meaning it does not assume a predefined form for the underlying data distribution and instead makes predictions based on the training examples themselves.\n",
        "- the working process of (KNN) in both classification and regression problems is written below:-\n",
        "- KNN in **Classification**\n",
        " -   You pick a number ùêæ (like 3 or 5).\n",
        " -  For the new point, you find the **K closest points** in your training data.\n",
        " -  You check what classes those neighbors belong to.\n",
        " -  The **majority class** among those neighbors becomes the prediction.\n",
        "\n",
        "- KNN in **Regression**\n",
        " -  Again, pick $K$.\n",
        " -  For the new point, find the **K nearest points** in the data.\n",
        " -  Instead of voting, take the **average (or weighted average)** of their values.\n",
        " -  That average becomes the prediction.\n",
        "\n",
        "\n",
        "### question-2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "- The curse of dimensionality refers to the problems that happen when working with data that has too many features (dimensions). like as the number of features grows, the data space becomes huge and points become very far apart, making it harder for algorithms like KNN to find \"meaningful neighbors.\n",
        "\n",
        "- it affects the performance of (KNN) in following ways:-\n",
        " - Distances between points become less meaningful ‚Üí hard to identify true neighbors.\n",
        " - Nearest and farthest neighbors appear almost equally distant ‚Üí weak neighbor distinction.\n",
        " - Data becomes sparse ‚Üí fewer reliable local neighbors to learn from.\n",
        " -  Model becomes more sensitive to noise and irrelevant features.\n",
        " - Increased computation time due to high-dimensional distance calculations.\n",
        " - Overall drop in accuracy for both classification and regression tasks.\n",
        "\n",
        "### question-3.What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "- principle componentt analysis (PCA) is a way of making data simple .like supose a dataset which have lots of features , many of these may overlap on each other.\n",
        "- (PCA) takes all these features and look for patterns then it creates a new feature summary or list in the data the first becomes largest second become second largest so that we can have only top few elements of data which makes data smaller and easier to work with while keeping data have its meaning.\n",
        "- difference from features selection is written below:-\n",
        " - PCA creates new features (principal components), while feature selection keeps original features.\n",
        " - PCA uses combinations of features to capture maximum variance, while feature selection just chooses the most useful ones.\n",
        " - PCA reduces dimensionality but loses interpretability, while feature selection keeps features easy to understand.\n",
        " - PCA is feature extraction, while feature selection is simply filtering.\n",
        "\n",
        "### question-4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "- Eigenvectors are the directions in which the data varies the most. In PCA, eigenvectors point to the axes of the new feature space (the principal components). Think of them as the ‚Äúcompass directions‚Äù that tell us where the most important patterns lie.\n",
        "\n",
        "- Eigenvalues tell us how much variance (or information) lies along each eigenvector. A larger eigenvalue means that direction captures more of the data‚Äôs spread.\n",
        "\n",
        "- its importance:-\n",
        " - Eigenvectors define the directions of maximum variance (new axes).\n",
        " - Eigenvalues measure the amount of variance captured by each direction.\n",
        " - They help rank principal components by importance.\n",
        " - They allow PCA to reduce dimensions while keeping most information.\n",
        "\n",
        "### question-5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "- KNN is a simple algorithm that makes predictions based on the distance between data points. Its performance heavily depends on meaningful distance calculations.\n",
        "- PCA is a dimensionality reduction technique that compresses high-dimensional data into fewer, more informative features (principal components).\n",
        "- When applied together in a pipeline:\n",
        " - PCA simplifies the data ‚Üí It removes noise and correlations between features by creating new compact components.\n",
        " - Distances become more meaningful ‚Üí KNN relies on distances, so after PCA, it works in a cleaner space where only the most important variation is kept.\n",
        " - Efficiency improves ‚Üí With fewer dimensions, KNN needs fewer calculations, making it faster.\n",
        " - Better accuracy and generalization ‚Üí PCA reduces the risk of overfitting, helping KNN focus on the strongest patterns instead of being distracted by redundant or irrelevant features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YZyJ2B6VZMhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:**\n",
        "- **Use the Wine Dataset from sklearn.datasets.load_wine().**\n",
        "\n",
        "###Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n"
      ],
      "metadata": {
        "id": "ZTGB_qPYi7p7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMFayT-yZCy1",
        "outputId": "bfb70da8-7c85-4799-9be4-9d2740fe641f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7222222222222222, 0.9444444444444444)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# spliting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "(acc_no_scaling, acc_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# question-7. : Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "JAlfOdVFj5Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2D-42Wnj44q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNrvGIK4ju9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}