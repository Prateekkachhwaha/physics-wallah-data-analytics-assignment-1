{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "######Assignment Code: DA-AG-017\n",
        "#Clustering | Assignment"
      ],
      "metadata": {
        "id": "xq_3iREry0xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###question-1 What is the difference between K-Means and Hierarchical Clustering? Provide a use case for each.\n",
        "### **K-Means Clustering**\n",
        "\n",
        "* Partitions data into **K clusters** using centroids.\n",
        "* **Requires** number of clusters in advance.\n",
        "* Works best with **spherical, equally sized clusters**.\n",
        "* Fast, scalable, but sensitive to **outliers**.\n",
        "* **Use Case:** Customer segmentation for marketing campaigns.\n",
        "\n",
        "### **Hierarchical Clustering**\n",
        "\n",
        "* Builds a **dendrogram** showing nested clusters.\n",
        "* **No need** to specify number of clusters initially.\n",
        "* Can handle **clusters of any shape/size**.\n",
        "* Computationally intensive for large datasets.\n",
        "* **Use Case:** Grouping genes with similar expression patterns.\n",
        "\n",
        "###question-2 Explain the purpose of the Silhouette Score in evaluating clustering algorithms.\n",
        "**Purpose:**\n",
        "\n",
        "* To evaluate the **quality of clustering** by measuring how similar each point is to its own cluster compared to other clusters.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* **a** = average distance to points in the same cluster (cohesion)\n",
        "* **b** = average distance to points in the nearest cluster (separation)\n",
        "* **Score formula:** ( s = \\frac{b - a}{\\max(a, b)} )\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* **s ≈ 1:** well-clustered\n",
        "* **s ≈ 0:** on cluster boundary\n",
        "* **s < 0:** possibly in wrong cluster\n",
        "\n",
        "**Use:**\n",
        "\n",
        "* Assess cluster quality\n",
        "* Help choose the **optimal number of clusters (K)**\n",
        "\n",
        "###question-3 What are the core parameters of DBSCAN, and how do they influence the clustering process?\n",
        "### **DBSCAN Core Parameters**\n",
        "\n",
        "* **ε (Epsilon):**\n",
        "\n",
        "  * Max distance to consider points as neighbors\n",
        "  * Small → many points labeled noise; Large → clusters may merge\n",
        "\n",
        "* **MinPts (Minimum Points):**\n",
        "\n",
        "  * Minimum points to form a dense region\n",
        "  * Small → more clusters, may include noise; Large → only dense areas form clusters\n",
        "\n",
        "* **Effect:**\n",
        "\n",
        "  * Determines cluster formation and noise detection\n",
        "  * Proper tuning is essential for meaningful clusters\n",
        "\n",
        "###question-4 Why is feature scaling important when applying clustering algorithms like K-Means and DBSCAN?\n",
        "### **Why Feature Scaling is Important in Clustering**\n",
        "\n",
        "* **Distance-based algorithms:**\n",
        "\n",
        "  * K-Means and DBSCAN use **distance metrics** (e.g., Euclidean distance) to group points.\n",
        "* **Prevent dominance of large-scale features:**\n",
        "\n",
        "  * Features with larger numerical ranges can **dominate the distance calculation**, skewing clustering.\n",
        "* **Ensure equal contribution:**\n",
        "\n",
        "  * Scaling (e.g., Min-Max or Standardization) gives all features **comparable influence** on clustering.\n",
        "* **Better cluster formation:**\n",
        "\n",
        "  * Leads to **more meaningful and balanced clusters**, avoiding bias toward certain features.\n",
        "\n",
        "###question-5 What is the Elbow Method in K-Means clustering and how does it help determine the optimal number of clusters?\n",
        "### **Elbow Method (K-Means Clustering)**\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "* To determine the **optimal number of clusters (K)**.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. Run K-Means for **different values of K** (e.g., 1 to 10).\n",
        "2. Calculate the **Within-Cluster Sum of Squares (WCSS)** for each K.\n",
        "\n",
        "   * WCSS = sum of squared distances of points to their cluster centroids.\n",
        "3. Plot **K vs. WCSS**.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "* Look for the **“elbow” point** where WCSS reduction **slows down sharply**.\n",
        "* This point indicates a **good balance** between cluster compactness and number of clusters.\n",
        "\n",
        "**Why it helps:**\n",
        "\n",
        "* Avoids overfitting (too many clusters) or underfitting (too few clusters).\n"
      ],
      "metadata": {
        "id": "rZTVRj6Hy-Td"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBVMMXrcyynK"
      },
      "outputs": [],
      "source": []
    }
  ]
}